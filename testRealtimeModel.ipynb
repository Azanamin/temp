{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import math\n",
    "\n",
    "from pose_estimation import utils\n",
    "from pose_estimation.data import BodyPart\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_estimation.ml import Movenet\n",
    "movenet = Movenet('movenet_thunder')\n",
    "\n",
    "# Gọi hàm nhận dạng của movenet nhiều lần để cải thiện độ chính xác nhận dạng khung xương\n",
    "def detect(input_tensor, inference_count=3):\n",
    "  \"\"\"Runs detection on an input image.\n",
    " \n",
    "  Args:\n",
    "    input_tensor: A [height, width, 3] Tensor of type tf.float32.\n",
    "    inference_count: Số lần lặp.\n",
    " \n",
    "  Returns:\n",
    "    A Person entity detected by the MoveNet.SinglePose.\n",
    "  \"\"\"\n",
    "  image_height, image_width, channel = input_tensor.shape\n",
    "\n",
    "  # Detect pose using the full input image\n",
    "  movenet.detect(input_tensor.numpy(), reset_crop_region=True)\n",
    "\n",
    "  # Repeatedly using previous detection result to identify the region of\n",
    "  # interest and only croping that region to improve detection accuracy\n",
    "  for _ in range(inference_count - 1):\n",
    "    person = movenet.detect(input_tensor.numpy(),\n",
    "                            reset_crop_region=False)\n",
    "\n",
    "  return person\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_point(landmarks, left_bodypart, right_bodypart):\n",
    "    \"\"\"Calculates the center point of the two given landmarks.\"\"\"\n",
    "\n",
    "    left = tf.gather(landmarks, left_bodypart.value, axis=1)\n",
    "    right = tf.gather(landmarks, right_bodypart.value, axis=1)\n",
    "    center = left * 0.5 + right * 0.5\n",
    "    return center\n",
    "\n",
    "\n",
    "def get_pose_size(landmarks, torso_size_multiplier=2.5):\n",
    "    \"\"\"Calculates pose size.\n",
    "\n",
    "    It is the maximum of two values:\n",
    "      * Torso size multiplied by `torso_size_multiplier`\n",
    "      * Maximum distance from pose center to any pose landmark\n",
    "    \"\"\"\n",
    "    # Hips center\n",
    "    hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP,\n",
    "                                   BodyPart.RIGHT_HIP)\n",
    "\n",
    "    # Shoulders center\n",
    "    shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,\n",
    "                                        BodyPart.RIGHT_SHOULDER)\n",
    "\n",
    "    # Torso size as the minimum body size\n",
    "    torso_size = tf.linalg.norm(shoulders_center - hips_center)\n",
    "\n",
    "    # Pose center\n",
    "    pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP,\n",
    "                                       BodyPart.RIGHT_HIP)\n",
    "    pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n",
    "    # Broadcast the pose center to the same size as the landmark vector to\n",
    "    # perform substraction\n",
    "    pose_center_new = tf.broadcast_to(pose_center_new,\n",
    "                                      [tf.size(landmarks) // (17*2), 17, 2])\n",
    "\n",
    "    # Dist to pose center\n",
    "    d = tf.gather(landmarks - pose_center_new, 0, axis=0,\n",
    "                  name=\"dist_to_pose_center\")\n",
    "    # Max dist to pose center\n",
    "    max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n",
    "\n",
    "    # Normalize scale\n",
    "    pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n",
    "\n",
    "    return pose_size\n",
    "\n",
    "\n",
    "def feature_pose(landmarks):\n",
    "    \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n",
    "    scaling it to a constant pose size.\n",
    "    \"\"\"\n",
    "    # Move landmarks so that the pose center becomes (0,0)\n",
    "    pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP,\n",
    "                                   BodyPart.RIGHT_HIP)\n",
    "    pose_center = tf.expand_dims(pose_center, axis=1)\n",
    "    # Broadcast the pose center to the same size as the landmark vector to perform\n",
    "    # substraction\n",
    "    pose_center = tf.broadcast_to(pose_center,\n",
    "                                  [tf.size(landmarks) // (17*2), 17, 2])\n",
    "    landmarks = landmarks - pose_center\n",
    "\n",
    "    # Scale the landmarks to a constant pose size\n",
    "    pose_size = get_pose_size(landmarks)\n",
    "    landmarks /= pose_size\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def normalize_drop_score(landmarks_and_scores):\n",
    "    \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n",
    "    # Reshape the flat input into a matrix with shape=(17, 3)\n",
    "    reshaped_inputs = tf.reshape(landmarks_and_scores, [1, 17, 3])\n",
    "\n",
    "    # Normalize landmarks 2D\n",
    "    norm_landmarks = feature_pose(reshaped_inputs[:, :, :2])\n",
    "\n",
    "    return norm_landmarks\n",
    "\n",
    "\n",
    "def angle_between_two_vector(a, b):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    inner = np.inner(a, b)\n",
    "    norms = LA.norm(a) * LA.norm(b)\n",
    "    cos = inner / norms\n",
    "    return np.arccos(np.clip(cos, -1.0, 1.0))\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    return x.numpy().flatten()\n",
    "\n",
    "\n",
    "def angle_between_three_point(landmarks, bodypart1, bodypart2, bodypart3, isBodyPart=True):\n",
    "    if isBodyPart:\n",
    "        bodypart1 = tf.gather(landmarks, bodypart1.value, axis=1)\n",
    "        bodypart2 = tf.gather(landmarks, bodypart2.value, axis=1)\n",
    "        bodypart3 = tf.gather(landmarks, bodypart3.value, axis=1)\n",
    "\n",
    "    v21 = bodypart1 - bodypart2\n",
    "    v23 = bodypart3 - bodypart2\n",
    "\n",
    "    # đơn vị radian (góc đúng cả 2 bên)\n",
    "    return angle_between_two_vector(flatten(v21), flatten(v23))\n",
    "\n",
    "\n",
    "def feature_angle(landmarks):\n",
    "    center_ear = get_center_point(\n",
    "        landmarks, BodyPart.LEFT_EAR, BodyPart.RIGHT_EAR)\n",
    "    center_shoulder = get_center_point(\n",
    "        landmarks, BodyPart.LEFT_SHOULDER, BodyPart.RIGHT_SHOULDER)\n",
    "    center_hip = get_center_point(\n",
    "        landmarks, BodyPart.LEFT_HIP, BodyPart.RIGHT_HIP)\n",
    "\n",
    "    angle_ear_shouler = angle_between_three_point(\n",
    "        landmarks, center_ear, center_shoulder, center_hip, False)\n",
    "\n",
    "    angle_left_torso_thighs = angle_between_three_point(\n",
    "        landmarks, BodyPart.LEFT_SHOULDER, BodyPart.LEFT_HIP, BodyPart.LEFT_KNEE)\n",
    "\n",
    "    angle_right_torso_thighs = angle_between_three_point(\n",
    "        landmarks, BodyPart.RIGHT_SHOULDER, BodyPart.RIGHT_HIP, BodyPart.RIGHT_KNEE)\n",
    "\n",
    "    angle_left_thighs_tibia = angle_between_three_point(\n",
    "        landmarks, BodyPart.LEFT_HIP, BodyPart.LEFT_KNEE, BodyPart.LEFT_ANKLE)\n",
    "\n",
    "    angle_right_thighs_tibia = angle_between_three_point(\n",
    "        landmarks, BodyPart.RIGHT_HIP, BodyPart.RIGHT_KNEE, BodyPart.RIGHT_ANKLE)\n",
    "\n",
    "    # print('angle between left torso and thighs: ',\n",
    "    #       np.rad2deg(angle_left_torso_thighs))\n",
    "\n",
    "    return [angle_ear_shouler, angle_left_torso_thighs, angle_left_thighs_tibia, angle_right_torso_thighs, angle_right_thighs_tibia]\n",
    "\n",
    "\n",
    "def get_distance(landmarks, left, right):\n",
    "    left = tf.gather(landmarks, left.value, axis=1)\n",
    "    right = tf.gather(landmarks, right.value, axis=1)\n",
    "\n",
    "    vector2 = (right - left).numpy()[0] ** 2\n",
    "\n",
    "    return math.sqrt(vector2[0] + vector2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(landmarks):\n",
    "    landmarks = normalize_drop_score(landmarks)\n",
    "    feature_vector = feature_pose(landmarks).numpy().flatten().tolist()\n",
    "    feature_vector.extend(feature_angle(landmarks))\n",
    "    feature_vector.append(get_distance(landmarks, BodyPart.LEFT_SHOULDER, BodyPart.RIGHT_SHOULDER))\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gac chan', 'Gu lung', 'Nga lung', 'Nghieng nguoi', 'Ngoi xom', 'Ngu gat', 'Thang lung']\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(interpreter, X):\n",
    "  \"\"\"Evaluates the given TFLite model and return its accuracy.\"\"\"\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on all given poses.\n",
    "  interpreter.set_tensor(input_index, X)\n",
    "\n",
    "  # Run inference.\n",
    "  interpreter.invoke()\n",
    "\n",
    "  # Post-processing: remove batch dimension and find the class with highest\n",
    "  # probability.\n",
    "  output = interpreter.tensor(output_index)\n",
    "\n",
    "  predicted_label = np.argmax(output()[0])\n",
    "\n",
    "  notes = \"\"\n",
    "  for i in range(len(output()[0])):\n",
    "    notes += f\"{class_names[i]}: {round(output()[0][i]*100/output()[0].sum(), 5)} %\\n\"\n",
    "\n",
    "  return predicted_label\n",
    "\n",
    "# TEST\n",
    "class_names = []\n",
    "with open('pose_labels.txt','r') as f:\n",
    "  for line in f.readlines():\n",
    "    class_names.append(line.strip())\n",
    "\n",
    "print(class_names)\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "# Evaluate the accuracy of the converted TFLite model\n",
    "classifier_interpreter = tf.lite.Interpreter(\n",
    "    model_path='pose_classifier.tflite')\n",
    "classifier_interpreter.allocate_tensors()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read()\n",
    "\n",
    "    if ret:\n",
    "      # Extract pose from image\n",
    "        tensor = tf.convert_to_tensor(image)\n",
    "        person = detect(tensor)\n",
    "\n",
    "        landmarks = []\n",
    "        for keypoint in person.keypoints:\n",
    "          landmarks.extend(\n",
    "              [keypoint.coordinate.x, keypoint.coordinate.y, keypoint.score])\n",
    "\n",
    "        feature = feature_extract(tf.constant([landmarks]))\n",
    "        feature = [list(map(np.float32, feature))]\n",
    "\n",
    "\n",
    "\n",
    "        y_pred = evaluate_model(classifier_interpreter, feature)\n",
    "\n",
    "        # Draw the detection result on top of the image.\n",
    "        image_np = utils.visualize(image, [person])\n",
    "\n",
    "        # Plot the image with detection results.\n",
    "        # offsetbox = TextArea(notes)\n",
    "\n",
    "        # ab = AnnotationBbox(offsetbox, (0.5, 0.7),\n",
    "        #                     xybox=(0.95, 0.5),\n",
    "        #                     # xycoords='data',\n",
    "        #                     boxcoords=(\"axes fraction\", \"data\"),\n",
    "        #                     # box_alignment=(0., 0.5),\n",
    "        #                     arrowprops=dict(arrowstyle=\"->\"))\n",
    "        # ax.add_artist(ab)\n",
    "\n",
    "        name = \"Detect person's sitting posture\"\n",
    "        cv2.namedWindow(name, cv2.WINDOW_KEEPRATIO)\n",
    "        cv2.putText(image_np, class_names[y_pred], (int(\n",
    "            image.shape[0]*0.35), int(image.shape[1]*0.5)), None, 1.5, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(name, image_np)\n",
    "        cv2.resizeWindow(name, 800, 500)\n",
    "\n",
    "        # Gõ q để tắt cam\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
